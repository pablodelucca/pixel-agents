# Pixel Agents -- Local LLM Configuration
# Copy this file to .env and update the values.
#
# This file is used by the standalone dev server (webview-ui/devServer.ts)
# and the localAgent.js process. In VS Code, the equivalent settings live
# under "Pixel Agents > Local" in the Settings UI.
#
# See the main README for full setup instructions:
#   https://github.com/LaansDole/local-pixel-agents#local-models-lm-studio

# These variables configure the built-in local agent that talks to any
# OpenAI-compatible endpoint (LM Studio, Ollama, vLLM, etc.) via the
# OpenAI chat completions API.

# Base URL of your local OpenAI-compatible server
PIXEL_AGENTS_BASE_URL=http://localhost:1234/v1

# API key for the local endpoint (LM Studio accepts any string)
PIXEL_AGENTS_API_KEY=lmstudio

# Model name to use (must match the model loaded in your local server)
PIXEL_AGENTS_MODEL=qwen2.5-coder-7b-instruct-mlx

# Maximum tokens per response (optional, default: 512)
# PIXEL_AGENTS_MAX_TOKENS=512
